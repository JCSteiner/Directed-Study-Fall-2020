{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DEPENDENCIES AND DEFINE CONSTANTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import numpy. Numpy is a library for python that will give us the ability to work with matricies and do things like matrix multiplication, the hadamard (element-wise) product, and transposing. This will also allow us to do inutitive things with matricies, like add and subtract them, without the need for nested for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define our constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first constant is EMBEDDING_DIMS this is how many dimensions we transform our inputs into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next constant is VOCAB_SIZE, this is how many possible inputs we could have, and how many outputs we will transform into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our third constant is NUM_HEADS, this is how many heads (concurrent sets of weights) our self-attention mechanism will have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fourth constant is NUM_BLOCKS, this is how many transformer blocks we will have. This will help us calculate more complex relationships between sets of sequences just like multiple layers would in any other type of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BLOCKS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMER FUNCTIONALITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to define is our embedding matrix. We'll call it embeddingMat. This is the matrix that will transform our inputs. The ith row in embeddingMat contains the input with index i's embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14855718,  0.62916748],\n",
       "       [ 0.47079458,  0.7360064 ],\n",
       "       [-0.23323845,  0.95891326],\n",
       "       [ 0.78638869, -0.58056966]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(27)\n",
    "\n",
    "#Defines our embedding matrix where each element is drawn from a random uniform distribution on the range [-1, 1) with\n",
    "#VOCAB_SIZE number of rows and EMBEDDING_DIMS number of columns\n",
    "embeddingMat = np.random.uniform(-1, 1, (VOCAB_SIZE, EMBEDDING_DIMS))\n",
    "\n",
    "#Let's take a look at what embeddingMat looks like\n",
    "embeddingMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention mechanisms have 3 ways to weight the input. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are called queries, keys, and values. Queries, keys, and values are all learned ways to weight your input vector. The queries and keys are then multiplied together to weight the output of the value matrix and the input vector. This is the self attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Instead of having a query, key, and value matrix for each attention head and then concatenating all the attention heads and transforming them back into the same dimensions, we have matricies that are NUM_HEADS times as tall as they need to be. In doing this, we pre-concatenate our values so we can be more memory efficient in transforming them back to the correct dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    #Defines our query, key, value, and unifyHeads matricies lists. We need a separate matrix for each block\n",
    "    queryMats = []\n",
    "    keyMats   = []\n",
    "    valueMats = []\n",
    "    unifyMats = []\n",
    "\n",
    "    #Defines the height of our pre-concatenated matricies\n",
    "    matHeight = EMBEDDING_DIMS * NUM_HEADS\n",
    "\n",
    "    #We perform the following operations, appending and intiailizing, NUM_BLOCKS number of times\n",
    "    for _ in range(NUM_BLOCKS):\n",
    "\n",
    "        #Defines our query, key, and value matricies\n",
    "        queryMats.append( np.random.uniform( -1, 1, ( matHeight, EMBEDDING_DIMS ) ) )\n",
    "        keyMats.append(   np.random.uniform( -1, 1, ( matHeight, EMBEDDING_DIMS ) ) )\n",
    "        valueMats.append(  np.random.uniform( -1, 1, ( matHeight, EMBEDDING_DIMS ) ) )\n",
    "\n",
    "        #Defines the matrix that will unify our heads, transforming our output into the correct dimensionality\n",
    "        unifyMats.append( np.random.uniform( -1, 1, ( EMBEDDING_DIMS, matHeight ) ) )\n",
    "\n",
    "    outputWeights = np.random.uniform(-1, 1, ( VOCAB_SIZE, EMBEDDING_DIMS ) )\n",
    "\n",
    "    #stores the values of queries, keys, and values for each block\n",
    "    blockQueries = []\n",
    "    blockKeys    = []\n",
    "    blockValues  = []\n",
    "\n",
    "    #stores the weighted values of the transformer blocks\n",
    "    weightedValues = []\n",
    "\n",
    "    #stores the weighted output values once they are passed through the unify heads layer\n",
    "    blockOutputs = []\n",
    "\n",
    "    #Initializes our gradients\n",
    "    dL_dZ              = np.zeros( VOCAB_SIZE )\n",
    "    dL_dOutputWeights  = np.zeros_like( outputWeights )\n",
    "    dL_dUnifyMats      = np.zeros_like( unifyMats[-1] )\n",
    "\n",
    "    #Initializes lists to store the values of the gradient of many weight matricies within the transformer block\n",
    "    dL_dUnifyMatsLst = []\n",
    "    dL_dQueriesLst   = []\n",
    "    dL_dKeysLst      = []\n",
    "    dL_dValuesLst    = []\n",
    "\n",
    "    #defaults our loss, we will add to this and average it out\n",
    "    loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define our output weights matrix. This will cast the output from our final transformer block back to the dimensionality of our vocab size. This allows us to make a prediction of our output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will define public lists that will store values needed for backpropagation through the network. If we had this in a typical class structure, then we would be defining these as instance variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Param()\n",
    "\n",
    "#We weill also define a function called zeroGrad. This function serves to reset these instance variables between forward passes\n",
    "def zeroGrad():\n",
    "\n",
    "    #zeros the values of queries, keys, and values for each block\n",
    "    model.blockQueries = []\n",
    "    model.blockKeys    = []\n",
    "    model.blockValues  = []\n",
    "\n",
    "    #zeros the weighted values of the transformer blocks\n",
    "    model.weightedValues = []\n",
    "\n",
    "    #zeros the weighted output values once they are passed through the unify heads layer\n",
    "    model.blockOutputs = []\n",
    "\n",
    "    #Initializes our gradients\n",
    "    model.dL_dZ              = np.zeros( VOCAB_SIZE )\n",
    "    model.dL_dOutputWeights  = np.zeros_like( model.outputWeights )\n",
    "    model.dL_dUnifyMats      = np.zeros_like( model.unifyMats[-1] )\n",
    "\n",
    "    #Initializes lists to store the values of the gradient of many weight matricies within the transformer block\n",
    "    model.dL_dUnifyMatsLst = []\n",
    "    model.dL_dQueriesLst   = []\n",
    "    model.dL_dKeysLst      = []\n",
    "    model.dL_dValuesLst    = []\n",
    "    \n",
    "    #zeros out our loss\n",
    "    model.loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next funtion we will have to define will be for our forward pass function. This will pass our input vector through each transformer block and will help us more clearly set up the transformer block structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define public lists that we will append to. They are needed for the learning algorithm we employ.\n",
    "#If this was in a class structure, they would be instance variables \n",
    "\n",
    "def fwdPass(inSeq, model):\n",
    "    # Param: inSeq - the sequence of labels we will pass into the transformer\n",
    "    \n",
    "    #First extracts the embedded input from the x sequence\n",
    "    embeddings = [ embeddingMat[x] for x in inSeq ]\n",
    "\n",
    "    #initializes the block output list\n",
    "    model.blockOutputs.append(embeddings)\n",
    "\n",
    "    #Loops through each block of the transformer\n",
    "    for block in range(NUM_BLOCKS):\n",
    "\n",
    "        #Initializes lists to store the queries, keys, and value vectors for each vector in the sequence x\n",
    "        queries = []\n",
    "        keys    = []\n",
    "        values  = []\n",
    "\n",
    "        #Loops through each x label in the input sequence passed into the transformer\n",
    "        for x in model.blockOutputs[ -1 ]:\n",
    "\n",
    "            #Calculates this embedded vectors query, key, and value\n",
    "            q = np.dot(model.queryMats[block], x)\n",
    "            k = np.dot(model.keyMats[block],   x)\n",
    "            v = np.dot(model.valueMats[block], x)\n",
    "\n",
    "            #Our scale factor is the square root of the query and key dimensionality\n",
    "            scaleFactor = EMBEDDING_DIMS ** (1/2)\n",
    "\n",
    "            #Before we save these vectors, we need to scale down the query and key vector by their scale factor\n",
    "            q /= scaleFactor\n",
    "            k /= scaleFactor\n",
    "\n",
    "            #Appends the calculated query, key and value vectors\n",
    "            queries.append( q )\n",
    "            keys.append(    k )\n",
    "            values.append(  v )\n",
    "\n",
    "        model.blockQueries.append(queries)\n",
    "        model.blockKeys.append(keys)\n",
    "        model.blockValues.append(values)\n",
    "\n",
    "        #Initializes the weight matrix for our weighted value vectors, we want to initialize it here so we can index\n",
    "        #into it without error below. We use the mathematical notation and loops for this to make an easier to understand\n",
    "        #implementation\n",
    "        weights = np.zeros( ( len( model.blockOutputs[ -1 ] ), len( model.blockOutputs[ -1 ] ) ) )\n",
    "\n",
    "        #Stores the weighted value vectors\n",
    "        y = []\n",
    "\n",
    "        #We want to compare each query to every other key and take their dot product, this will give us raw self attention\n",
    "        for i, q in enumerate(queries):\n",
    "            for j, k in enumerate(keys):\n",
    "                #compares each query q to ever other key j\n",
    "                weights[i][j] = np.dot(q, k)\n",
    "\n",
    "        #We already calculated our raw weights, now we need to normalize those weights by passing them through the softmax function\n",
    "        weights = np.exp(weights) / np.sum(np.exp(weights), axis = 1)\n",
    "        weightedValuesBlock = []\n",
    "        #For each value vector\n",
    "        for i in range(len(inSeq)):\n",
    "            \n",
    "            #initializes the weighted values as an array of zeros\n",
    "            weightedValue = np.zeros_like(values[i])\n",
    "\n",
    "            #sums the weights of values over j\n",
    "            for j in range(len(inSeq)):\n",
    "\n",
    "                #We first calculate the weighted value vector\n",
    "                weightedValue += weights[i][j] * values[j]\n",
    "\n",
    "            #Appends to the weighted values list\n",
    "            weightedValuesBlock.append( weightedValue )\n",
    "\n",
    "            #Append the weighted value vector to the y vector\n",
    "            y.append( np.dot( model.unifyMats[block], weightedValue ) )\n",
    "\n",
    "        \n",
    "        #TODO normalize layer\n",
    "\n",
    "        model.weightedValues.append(weightedValuesBlock)\n",
    "        #at the end of the block, append the block output to the block outputs list\n",
    "        model.blockOutputs.append(y)\n",
    "\n",
    "    #stores the output vectors of the entire transformer\n",
    "    modelOutputs = []\n",
    "\n",
    "    #Loops through each y vector in the block outputs we have just appended to\n",
    "    for y in model.blockOutputs[ -1 ]:\n",
    "        \n",
    "        #The intermediate activation of casting this y vector to the \n",
    "        activation = np.dot( model.outputWeights, y )\n",
    "\n",
    "        #Appends to the output vector after applying the softmax activation function to it\n",
    "        modelOutputs.append( np.exp( activation ) / np.sum( np.exp( activation ) ) )\n",
    "    \n",
    "    #TODO possible layer normalization\n",
    "\n",
    "    #Return the model output\n",
    "    return modelOutputs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now completed the function to calculate our forward pass. Now let's look at our output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we should expect, fairly meaningless output without training. We have probabilities of what each sequence is in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkwdPass(predicted, targets):\n",
    "\n",
    "    model.loss = 0.0\n",
    "\n",
    "    #For each target in our target sequence\n",
    "    for i, target in enumerate(targets):\n",
    "\n",
    "        #Converts our target into a one hot vector\n",
    "        t = np.zeros( VOCAB_SIZE )\n",
    "        t[ target ] = 1\n",
    "\n",
    "        #Increments loss\n",
    "        model.loss += -np.sum( t * np.log(predicted[i]) )\n",
    "\n",
    "        #Increments the loss with respect to the outputs\n",
    "        model.dL_dZ += predicted[i] - t\n",
    "\n",
    "    #Loops through the length of the final output weights\n",
    "    for i in range(VOCAB_SIZE):\n",
    "        for j in range(EMBEDDING_DIMS):\n",
    "            #Loops through the each vector we had a prediction for\n",
    "            for yhat in predicted:\n",
    "                model.dL_dOutputWeights[i][j] += model.dL_dZ[i] * yhat[j]\n",
    "\n",
    "    #We want to average over all of our predicted vectors, so we still need to divide each weight by the number of predicted vectors we have\n",
    "    model.dL_dOutputWeights /= len(predicted)\n",
    "\n",
    "    dL_dY = np.dot(model.outputWeights.transpose(), model.dL_dZ)\n",
    "\n",
    "    for block in range(1, NUM_BLOCKS + 1):\n",
    "\n",
    "        for i in range(EMBEDDING_DIMS):\n",
    "            for j in range(EMBEDDING_DIMS * NUM_HEADS):\n",
    "                for t in range(2):\n",
    "                    model.dL_dUnifyMats[i][j] += dL_dY[i] * model.weightedValues[-block][t][j]\n",
    "\n",
    "        model.dL_dUnifyMats /= len(model.blockOutputs)\n",
    "\n",
    "        model.dL_dUnifyMatsLst.append( model.dL_dUnifyMats )\n",
    "\n",
    "        dL_dValueOut = np.dot(model.unifyMats[-block].transpose(), dL_dY)\n",
    "\n",
    "        model.dL_dQueries = np.zeros_like(model.blockQueries[-block][0])\n",
    "        model.dL_dKeys    = np.zeros_like(model.blockKeys[-block][0])\n",
    "        model.dL_dValues  = np.zeros_like(model.blockValues[-block][0])\n",
    "\n",
    "        for q, k, v in zip(model.blockQueries[-block], model.blockKeys[-block], model.blockValues[-block]):            \n",
    "            model.dL_dQueries += k * v * dL_dValueOut\n",
    "            model.dL_dKeys    += q * v * dL_dValueOut\n",
    "            model.dL_dValues  += q * k * dL_dValueOut\n",
    "        \n",
    "        #Averages out the gradients\n",
    "        model.dL_dQueries /= len(model.blockQueries[-block])\n",
    "        model.dL_dKeys  /= len(model.blockKeys[-block])\n",
    "        model.dL_dValues /= len(model.blockValues[-block])\n",
    "\n",
    "        model.dL_dQueryWeights = np.zeros_like(model.queryMats[-block])\n",
    "        model.dL_dKeyWeights   = np.zeros_like(model.keyMats[-block])\n",
    "        model.dL_dValueWeights = np.zeros_like(model.valueMats[-block])\n",
    "\n",
    "        for i in range(NUM_HEADS * EMBEDDING_DIMS):\n",
    "            for j in range(EMBEDDING_DIMS):\n",
    "                for t in model.blockOutputs[-block - 1]:\n",
    "                    model.dL_dQueryWeights[i][j] += model.dL_dQueries[i] * t[j]\n",
    "                    model.dL_dKeyWeights[i][j] += model.dL_dQueries[i] * t[j]\n",
    "                    model.dL_dValueWeights[i][j] += model.dL_dQueries[i] * t[j]\n",
    "\n",
    "        model.dL_dQueryWeights /= len(x)\n",
    "        model.dL_dKeyWeights /= len(x)\n",
    "        model.dL_dValueWeights /= len(x)\n",
    "\n",
    "        # dL_dEmbeddings = np.zeros_like(embeddingMat)\n",
    "    \n",
    "\n",
    "        model.dL_dUnifyMatsLst.append(model.dL_dUnifyMats)\n",
    "        model.dL_dQueriesLst.append(model.dL_dQueryWeights)\n",
    "        model.dL_dKeysLst.append(model.dL_dKeyWeights)\n",
    "        model.dL_dValuesLst.append(model.dL_dValueWeights)\n",
    "\n",
    "    learningRate = 0.1\n",
    "\n",
    "    model.dL_dUnifyMatsLst.reverse()\n",
    "    model.dL_dQueriesLst.reverse()\n",
    "    model.dL_dKeysLst.reverse()\n",
    "    model.dL_dValuesLst.reverse()\n",
    "\n",
    "    for block in range(NUM_BLOCKS):\n",
    "\n",
    "        model.queryMats[block] -= learningRate * model.dL_dQueriesLst[block]\n",
    "        model.keyMats[block] -= learningRate * model.dL_dKeysLst[block]\n",
    "        model.valueMats[block] -= learningRate * model.dL_dValuesLst[block]\n",
    "\n",
    "        model.unifyMats[block] -= learningRate * model.dL_dUnifyMatsLst[block]\n",
    "\n",
    "    model.outputWeights -= learningRate * model.dL_dOutputWeights\n",
    "\n",
    "    return model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [0, 0] Actual: [3, 3]\n",
      "Training...\n",
      "Cost: 2.7784030707347434\n",
      "Cost: 2.49628796230576\n",
      "Cost: 2.811686719639754\n",
      "Cost: 7.543598639370524\n",
      "Cost: 1.4020167883348268e-05\n",
      "Cost: 1.3934810031764254e-05\n",
      "Cost: 1.3850433254291826e-05\n",
      "Cost: 1.3767021031684722e-05\n",
      "Cost: 1.3684557209744674e-05\n",
      "Cost: 1.360302599066146e-05\n",
      "Predicted: [3, 3] Actual: [3, 3]\n"
     ]
    }
   ],
   "source": [
    "#Initializes a sequence of x labels\n",
    "x = [0, 0]\n",
    "targets = [3, 3]\n",
    "np.random.seed(27)\n",
    "zeroGrad()\n",
    "pred = fwdPass(x, model)\n",
    "print(\"Predicted:\", [np.argmax(x) for x in pred], \"Actual:\", targets)\n",
    "print(\"Training...\")\n",
    "for _ in range(100):\n",
    "    zeroGrad()\n",
    "    pred = fwdPass(x, model)\n",
    "    cost = bkwdPass(pred, targets)\n",
    "    if _ % 10 == 0:\n",
    "        print(\"Cost:\", cost)\n",
    "pred = fwdPass(x, model)\n",
    "print(\"Predicted:\", [np.argmax(x) for x in pred], \"Actual:\", targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
